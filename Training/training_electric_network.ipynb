{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the electric network\n",
    "\n",
    "Write a two-layer neural network to train image in raw format. The network is trained to predict the electric network from the image. The network is trained on the training set and evaluated on the test set. The network is trained using stochastic gradient descent with a fixed learning rate and no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import raw image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. have a feel of the raw image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10467 12525 13927 ...  7098  7706  7925]\n",
      " [12844 13906  9647 ...  6527  6522  6164]\n",
      " [13717 12368  6016 ...  7291  7276  6490]\n",
      " ...\n",
      " [ 7560  4947  4667 ...  7730  7484  5078]\n",
      " [ 8558  5854  6281 ...  8016  7724  5039]\n",
      " [ 7936  6199  7346 ...  7341  6705  4643]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open the raw image file\n",
    "with open('test_image/pic_20231025_15 1 2_991.raw', 'rb') as f:\n",
    "    # read the raw image data\n",
    "    raw_data = f.read()\n",
    "\n",
    "# create a PIL image from the raw data\n",
    "width, height = 32, 32\n",
    "img = Image.frombytes('I;16', (width, height), raw_data, decoder_name='raw')\n",
    "# img.show()\n",
    "\n",
    "# convert the PIL image to a numpy array\n",
    "image = np.array(img)\n",
    "print(image)\n",
    "\n",
    "# display the image\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(image, cmap='gray', vmin=0, vmax=2**14-1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. make a dataset\n",
    "3 kinds of objects in 3 folders, 250 raw images in each folder, labeled as 0, 1, 2.\n",
    "\n",
    "`datasets.ImageFolder(root='./data', transform=transform)` do not support raw image, so we need to make a dataset by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the path to the root folder of the dataset\n",
    "root = '红外光测试'\n",
    "\n",
    "# define the transformations to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1, 32*32)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# create the Image dataset with only raw images\n",
    "dataset = datasets.ImageFolder(root=root, transform=transform, )\n",
    "\n",
    "# print the class labels\n",
    "print('dataset.class:', dataset.classes)\n",
    "\n",
    "# add label to images in each folder\n",
    "dataset.class_to_idx = {'img32x32_20231023_113512_flight': 0, \n",
    "                        'img32x32_20231023_113737_oil': 1, \n",
    "                        'img32x32_20231023_113921_boat': 2}\n",
    "\n",
    "# print an example in dataset\n",
    "print(dataset[1], '\\n', \n",
    "    dataset[1][0].shape, '\\n',\n",
    "    dataset[1][1])\n",
    "\n",
    "\n",
    "# print the number of images in the dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# # print the number of images per class\n",
    "# print(dataset.class_counts)\n",
    "training_data = DataLoader(dataset, batch_size=25, shuffle=True)\n",
    "train_dataloader = DataLoader(training_data, batch_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the path to the root folder of the dataset\n",
    "root = '红外光测试'\n",
    "\n",
    "# define the transformations to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1, 32*32)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# create the Image dataset with only raw images\n",
    "dataset = datasets.ImageFolder(root=root, transform=transform, )\n",
    "\n",
    "# print the class labels\n",
    "print('dataset.class:', dataset.classes)\n",
    "\n",
    "# add label to images in each folder\n",
    "dataset.class_to_idx = {'img32x32_20231023_113512_flight': 0, \n",
    "                        'img32x32_20231023_113737_oil': 1, \n",
    "                        'img32x32_20231023_113921_boat': 2}\n",
    "\n",
    "# print an example in dataset\n",
    "print(dataset[1], '\\n', \n",
    "    dataset[1][0].shape, '\\n',\n",
    "    dataset[1][1])\n",
    "\n",
    "\n",
    "# print the number of images in the dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# # print the number of images per class\n",
    "# print(dataset.class_counts)\n",
    "training_data = DataLoader(dataset, batch_size=25, shuffle=True)\n",
    "train_dataloader = DataLoader(training_data, batch_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. define the network\n",
    "\n",
    "The input tensor is a 32 * 32 raw image and flatten to 1 * 1024.\n",
    "\n",
    "The weight value in the 1st layer should be binary (+1 or -1), the dimension of the weight matrix is 1024*32. The activation function is relu function.\n",
    "\n",
    "The weight value in the 2nd layer signed 16 bit, the dimension of the weight matrix is 32 *3. The output is 1 *3 matrix. The activation function is softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "# The weight value in the 1st layer should be binary (+1 or -1), \n",
    "# the dimension of the weight matrix is 1024*32. The activation function is relu function.\n",
    "# The weight value in the 2nd layer signed 16 bit, the dimension of the weight matrix is 32 *3. \n",
    "# The output is 1 *3 matrix. The activation function is softmax function.\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 32)\n",
    "        self.fc2 = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    # def binary(self):\n",
    "    #     self.fc1.weight.data = torch.where(self.fc1.weight.data > 0, \n",
    "    #             torch.ones_like(self.fc1.weight.data),\n",
    "    #             -torch.ones_like(self.fc1.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhouzhang/Library/CloudStorage/坚果云-zhouzhang1992@qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzhang/Library/CloudStorage/%E5%9D%9A%E6%9E%9C%E4%BA%91-zhouzhang1992%40qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Set the model to training mode - important for batch normalization and dropout layers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzhang/Library/CloudStorage/%E5%9D%9A%E6%9E%9C%E4%BA%91-zhouzhang1992%40qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Unnecessary in this situation but added for best practices\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzhang/Library/CloudStorage/%E5%9D%9A%E6%9E%9C%E4%BA%91-zhouzhang1992%40qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouzhang/Library/CloudStorage/%E5%9D%9A%E6%9E%9C%E4%BA%91-zhouzhang1992%40qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzhang/Library/CloudStorage/%E5%9D%9A%E6%9E%9C%E4%BA%91-zhouzhang1992%40qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzhang/Library/CloudStorage/%E5%9D%9A%E6%9E%9C%E4%BA%91-zhouzhang1992%40qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzhang/Library/CloudStorage/%E5%9D%9A%E6%9E%9C%E4%BA%91-zhouzhang1992%40qq.com/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 25\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for batch, (X, y) in enumerate(train_dataloader):\n",
    "#     print(batch)\n",
    "size = len(train_dataloader.dataset)\n",
    "# Set the model to training mode - important for batch normalization and dropout layers\n",
    "# Unnecessary in this situation but added for best practices\n",
    "model.train()\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    # Compute prediction and loss\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
