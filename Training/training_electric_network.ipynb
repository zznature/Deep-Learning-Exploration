{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the electric network\n",
    "\n",
    "Write a two-layer neural network to train image in raw format. The network is trained to predict the electric network from the image. The network is trained on the training set and evaluated on the test set. The network is trained using stochastic gradient descent with a fixed learning rate and no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import raw image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. have a feel of the raw image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawimage2tensor(raw_data, width=32, height=32):\n",
    "    # create a PIL image from the raw data\n",
    "    img = Image.frombytes('I;16', (width, height), raw_data, decoder_name='raw')\n",
    "\n",
    "    # convert the PIL image to a numpy array\n",
    "    image = np.array(img)\n",
    "\n",
    "    # convert uint16 to int16\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    # convert the numpy array to a tensor\n",
    "    tensor = torch.from_numpy(image)\n",
    "\n",
    "    # return the tensor\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 read the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the raw image file\n",
    "# with open('test_image/pic_20231025_15 1 2_991.raw', 'rb') as f:\n",
    "#     # read the raw image data\n",
    "#     raw_data = f.read()\n",
    "\n",
    "# initialize the dataset\n",
    "dataset = []\n",
    "\n",
    "# open the raw image file in folder '红外光测试'\n",
    "for image in os.listdir('红外光测试/img32x32_20231023_113512_flight/'):\n",
    "    with open('红外光测试/img32x32_20231023_113512_flight/' + image, 'rb') as f:\n",
    "        # read the raw image data\n",
    "        raw_data = f.read()\n",
    "        matrix = rawimage2tensor(raw_data)\n",
    "        label = 0  # 0 for flight\n",
    "        # append the image tensor into dataset\n",
    "        dataset.append((matrix, label))\n",
    "\n",
    "for image in os.listdir('红外光测试/img32x32_20231023_113737_oil/'):\n",
    "    with open('红外光测试/img32x32_20231023_113737_oil/' + image, 'rb') as f:\n",
    "        # read the raw image data\n",
    "        raw_data = f.read()\n",
    "        matrix = rawimage2tensor(raw_data)\n",
    "        label = 1  # 1 for oil-tank\n",
    "        # append the image tensor into dataset\n",
    "        dataset.append((matrix, label))\n",
    "\n",
    "for image in os.listdir('红外光测试/img32x32_20231023_113921_boat/'):\n",
    "    with open('红外光测试/img32x32_20231023_113921_boat/' + image, 'rb') as f:\n",
    "        # read the raw image data\n",
    "        raw_data = f.read()\n",
    "        matrix = rawimage2tensor(raw_data)\n",
    "        label = 2  # 2 for boat\n",
    "        # append the image tensor into dataset\n",
    "        dataset.append((matrix, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "torch.Size([32, 32])\n",
      "tensor([[4268, 4277, 4430,  ..., 5454, 4849, 3208],\n",
      "        [6667, 6710, 7087,  ..., 7364, 6053, 3205],\n",
      "        [6437, 6295, 6864,  ..., 5828, 4586, 3230],\n",
      "        ...,\n",
      "        [4858, 5170, 4752,  ..., 4095, 2901, 2748],\n",
      "        [4840, 5028, 4715,  ..., 4387, 2953, 2824],\n",
      "        [5001, 5118, 4673,  ..., 4204, 2941, 2878]], dtype=torch.int16) 0\n",
      "tensor([[3093, 3141, 3113,  ..., 3373, 3531, 3130],\n",
      "        [5696, 5946, 5944,  ..., 5403, 4834, 3227],\n",
      "        [7764, 8151, 7958,  ..., 4892, 3996, 3195],\n",
      "        ...,\n",
      "        [4893, 5218, 4895,  ..., 5048, 2994, 2895],\n",
      "        [5022, 5135, 4731,  ..., 5090, 2984, 3049],\n",
      "        [5695, 5389, 4712,  ..., 5161, 3109, 2901]], dtype=torch.int16) 1\n",
      "tensor([[3133, 3069, 3135,  ..., 3473, 3358, 2828],\n",
      "        [4175, 4059, 4011,  ..., 4316, 3753, 2748],\n",
      "        [4432, 4171, 4388,  ..., 3687, 3397, 2807],\n",
      "        ...,\n",
      "        [3567, 3836, 3459,  ..., 3144, 2628, 2567],\n",
      "        [3515, 3587, 3400,  ..., 3240, 2666, 2670],\n",
      "        [3376, 3302, 3462,  ..., 3331, 2688, 2687]], dtype=torch.int16) 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cd7485fcd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(dataset[0][0].shape)\n",
    "# check a random sample in dataset\n",
    "sample = dataset[0]\n",
    "print(sample[0], sample[1])\n",
    "plt.imshow(sample[0], cmap='gray')\n",
    "\n",
    "sample = dataset[250]\n",
    "print(sample[0], sample[1])\n",
    "plt.imshow(sample[0], cmap='gray')\n",
    "\n",
    "sample = dataset[500]\n",
    "print(sample[0], sample[1])\n",
    "plt.imshow(sample[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. make a dataset\n",
    "3 kinds of objects in 3 folders, 250 raw images in each folder, labeled as 0, 1, 2.\n",
    "\n",
    "`datasets.ImageFolder(root='./data', transform=transform)` do not support raw image, so we need to make a dataset by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zhouz\\Nutstore\\1\\Code\\Deep-Learning-Exploration\\Training\\training_electric_network.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m root \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m红外光测试\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# create the Image dataset with only raw images\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mImageFolder(root\u001b[39m=\u001b[39mroot, transform\u001b[39m=\u001b[39mtransform, )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print the class labels\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdataset.class:\u001b[39m\u001b[39m'\u001b[39m, dataset\u001b[39m.\u001b[39mclasses)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the path to the root folder of the dataset\n",
    "root = '红外光测试'\n",
    "\n",
    "# create the Image dataset with only raw images\n",
    "dataset = datasets.ImageFolder(root=root, transform=transform, )\n",
    "\n",
    "# print the class labels\n",
    "print('dataset.class:', dataset.classes)\n",
    "\n",
    "# add label to images in each folder\n",
    "dataset.class_to_idx = {'img32x32_20231023_113512_flight': 0, \n",
    "                        'img32x32_20231023_113737_oil': 1, \n",
    "                        'img32x32_20231023_113921_boat': 2}\n",
    "\n",
    "# print an example in dataset\n",
    "print(dataset[1], '\\n', \n",
    "    dataset[1][0].shape, '\\n',\n",
    "    dataset[1][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # print the number of images per class\n",
    "# print(dataset.class_counts)\n",
    "training_dataloader = DataLoader(dataset, batch_size=25, shuffle=True)\n",
    "# train_dataloader = DataLoader(training_data, batch_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. define the network\n",
    "\n",
    "The input tensor is a 32 * 32 raw image and flatten to 1 * 1024.\n",
    "\n",
    "The weight value in the 1st layer should be binary (+1 or -1), the dimension of the weight matrix is 1024*32. The activation function is relu function.\n",
    "\n",
    "The weight value in the 2nd layer signed 16 bit, the dimension of the weight matrix is 32 *3. The output is 1 *3 matrix. The activation function is softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "# The weight value in the 1st layer should be binary (+1 or -1), \n",
    "# the dimension of the weight matrix is 1024*32. The activation function is relu function.\n",
    "# The weight value in the 2nd layer signed 16 bit, the dimension of the weight matrix is 32 *3. \n",
    "# The output is 1 *3 matrix. The activation function is softmax function.\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 32)\n",
    "        self.fc2 = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    # def binary(self):\n",
    "    #     self.fc1.weight.data = torch.where(self.fc1.weight.data > 0, \n",
    "    #             torch.ones_like(self.fc1.weight.data),\n",
    "    #             -torch.ones_like(self.fc1.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zhouz\\Nutstore\\1\\Code\\Deep-Learning-Exploration\\Training\\training_electric_network.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Set the model to training mode - important for batch normalization and dropout layers\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Unnecessary in this situation but added for best practices\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouz/Nutstore/1/Code/Deep-Learning-Exploration/Training/training_electric_network.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 25\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for batch, (X, y) in enumerate(train_dataloader):\n",
    "#     print(batch)\n",
    "size = len(train_dataloader.dataset)\n",
    "# Set the model to training mode - important for batch normalization and dropout layers\n",
    "# Unnecessary in this situation but added for best practices\n",
    "model.train()\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    # Compute prediction and loss\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
