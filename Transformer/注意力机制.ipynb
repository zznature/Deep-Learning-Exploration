{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制\n",
    "\n",
    "意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。\n",
    "\n",
    "自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力框架\n",
    "回顾一个经典注意力框架，解释如何在视觉场景中展开注意力。 受此框架中的注意力提示（attention cues）的启发， 我们将设计能够利用这些注意力提示的模型。 1964年的Nadaraya-Waston核回归（kernel regression）正是具有 注意力机制（attention mechanism）的机器学习的简单演示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力函数\n",
    "\n",
    "然后继续介绍的是注意力函数，它们在深度学习的注意力模型设计中被广泛使用。 具体来说，我们将展示如何使用这些函数来设计Bahdanau注意力。 Bahdanau注意力是深度学习中的具有突破性价值的注意力模型，它双向对齐并且可以微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [softmax回归](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#softmax)\n",
    "\n",
    "在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。\n",
    "\n",
    "通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer架构\n",
    "\n",
    "最后将描述仅仅基于注意力机制的Transformer架构， 该架构中使用了多头注意力（multi-head attention） 和自注意力（self-attention）。 自2017年横空出世，Transformer一直都普遍存在于现代的深度学习应用中， 例如语言、视觉、语音和强化学习领域。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
